using world size: 4 and model-parallel size: 4 
using torch.float16 for parameters ...
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:BertWordPieceLowerCase
-------------------- arguments --------------------
  adam_beta1 ...................... 0.9
  adam_beta2 ...................... 0.999
  adam_eps ........................ 1e-08
  adlr_autoresume ................. False
  adlr_autoresume_interval ........ 1000
  apply_query_key_layer_scaling ... False
  apply_residual_connection_post_layernorm  False
  attention_dropout ............... 0.1
  attention_softmax_in_fp32 ....... False
  batch_size ...................... 1
  bert_load ....................... None
  bias_dropout_fusion ............. False
  bias_gelu_fusion ................ False
  block_data_path ................. None
  checkpoint_activations .......... True
  checkpoint_in_cpu ............... False
  checkpoint_num_layers ........... 1
  clip_grad ....................... 1.0
  contigious_checkpointing ........ False
  cpu_optimizer ................... False
  cpu_torch_adam .................. False
  data_impl ....................... mmap
  data_path ....................... /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence
  DDP_impl ........................ local
  deepscale ....................... False
  deepscale_config ................ None
  deepspeed ....................... False
  deepspeed_activation_checkpointing  False
  deepspeed_config ................ None
  deepspeed_mpi ................... False
  distribute_checkpointed_activations  False
  distributed_backend ............. nccl
  dynamic_loss_scale .............. True
  eod_mask_loss ................... False
  eval_interval ................... 1000
  eval_iters ...................... 10
  exit_interval ................... None
  faiss_use_gpu ................... False
  finetune ........................ False
  fp16 ............................ True
  fp16_lm_cross_entropy ........... False
  fp32_allreduce .................. False
  hidden_dropout .................. 0.1
  hidden_size ..................... 3072
  hysteresis ...................... 2
  ict_head_size ................... None
  ict_load ........................ None
  indexer_batch_size .............. 128
  indexer_log_interval ............ 1000
  init_method_std ................. 0.02
  layernorm_epsilon ............... 1e-05
  lazy_mpu_init ................... None
  load ............................ /home/asc22g0/ASC22/DeepSpeed/DeepSpeedExamples/Megatron-LM-v1.1.5-ZeRO3/checkpoints/ds_gpt2_4.7B
  local_rank ...................... 0
  log_interval .................... 1
  loss_scale ...................... None
  loss_scale_window ............... 1000
  lr .............................. 0.00015
  lr_decay_iters .................. 320000
  lr_decay_style .................. cosine
  make_vocab_size_divisible_by .... 128
  mask_prob ....................... 0.15
  max_position_embeddings ......... 2048
  memory_centric_tiled_linear ..... False
  merge_file ...................... None
  min_lr .......................... 1e-05
  min_scale ....................... 1
  mmap_warmup ..................... False
  model_parallel_size ............. 4
  no_load_optim ................... False
  no_load_rng ..................... False
  no_save_optim ................... False
  no_save_rng ..................... False
  num_attention_heads ............. 24
  num_layers ...................... 40
  num_unique_layers ............... None
  num_workers ..................... 2
  onnx_safe ....................... None
  openai_gelu ..................... False
  override_lr_scheduler ........... False
  param_sharing_style ............. grouped
  params_dtype .................... torch.float16
  partition_activations ........... False
  profile_backward ................ False
  query_in_block_prob ............. 0.1
  rank ............................ 0
  remote_device ................... none
  report_topk_accuracies .......... []
  reset_attention_mask ............ False
  reset_position_ids .............. False
  save ............................ /home/asc22g0/ASC22/DeepSpeed/DeepSpeedExamples/Megatron-LM-v1.1.5-ZeRO3/checkpoints/ds_gpt2_4.7B
  save_interval ................... 10000
  scaled_masked_softmax_fusion .... False
  scaled_upper_triang_masked_softmax_fusion  False
  scattered_embeddings ............ False
  seed ............................ 1234
  seq_length ...................... 2048
  short_seq_prob .................. 0.1
  split ........................... 949,50,1
  split_transformers .............. False
  synchronize_each_layer .......... False
  tensorboard_dir ................. None
  tile_factor ..................... 1
  titles_data_path ................ None
  tokenizer_type .................. BertWordPieceLowerCase
  tokens .......................... 0
  train_iters ..................... 500000
  train_tokens .................... 10000000
  use_checkpoint_lr_scheduler ..... False
  use_cpu_initialization .......... False
  use_one_sent_docs ............... False
  use_pin_memory .................. False
  vocab_file ...................... /home/asc22g0/ASC22/vocab_dxy.txt
  warmup .......................... 0.01
  warmup_iters .................... None
  weight_decay .................... 0.01
  world_size ...................... 4
  zero_allgather_bucket_size ...... 0.0
  zero_contigious_gradients ....... False
  zero_reduce_bucket_size ......... 0.0
  zero_reduce_scatter ............. False
  zero_stage ...................... 1.0
---------------- end of arguments ----------------
> building BertWordPieceLowerCase tokenizer ...
 > padded vocab (size: 53229) with 19 dummy tokens (new size: 53248)
> initializing torch distributed ...
> initializing model parallel with size 4
> setting random seeds to 1234 ...
> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
building GPT2 model ...
[2022-03-04 15:10:36,729] [INFO] [utils.py:822:see_memory_usage] Before Building Model
[2022-03-04 15:10:36,730] [INFO] [utils.py:827:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2022-03-04 15:10:36,730] [INFO] [utils.py:832:see_memory_usage] CPU Virtual Memory:  used = 79.05 GB, percent = 31.4%
 > number of parameters on model parallel rank 3            4.722 Billion
 > number of parameters on model parallel rank 2            4.722 Billion
[2022-03-04 15:10:36,983] [INFO] [utils.py:822:see_memory_usage] After Building Model
[2022-03-04 15:10:36,984] [INFO] [utils.py:827:see_memory_usage] MA 2.19 GB         Max_MA 2.19 GB         CA 2.28 GB         Max_CA 2 GB 
[2022-03-04 15:10:36,984] [INFO] [utils.py:832:see_memory_usage] CPU Virtual Memory:  used = 79.15 GB, percent = 31.4%
 > number of parameters on model parallel rank 0            4.722 Billion
 > number of parameters on model parallel rank 1            4.722 Billion
> learning rate decay style: cosine
WARNING: could not find the metadata file /home/asc22g0/ASC22/DeepSpeed/DeepSpeedExamples/Megatron-LM-v1.1.5-ZeRO3/checkpoints/ds_gpt2_4.7B/latest_checkpointed_iteration.txt 
    will not load any checkpoints and will start from random
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      500000
    validation: 5010
    test:       10
> building train, validation, and test datasets for GPT2 ...
 > building dataset index ...
    reading sizes...
    reading pointers...
    reading document index...
    creating numpy buffer of mmap...
    creating memory view of numpy buffer...
 > finished creating indexed dataset in 0.001498 seconds
    number of documents: 1706760
 > dataset split:
    train:
     document indices in [0, 1619715) total of 1619715 documents
    validation:
     document indices in [1619715, 1705053) total of 85338 documents
    test:
     document indices in [1705053, 1706760) total of 1707 documents
 > loading doc-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_train_indexmap_500000ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_train_indexmap_500000ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_train_indexmap_500000ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 557359
    total number of epochs: 1
 > loading doc-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_valid_indexmap_5010ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_valid_indexmap_5010ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_valid_indexmap_5010ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.018 seconds
    total number of samples: 62129
    total number of epochs: 1
 > loading doc-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_test_indexmap_10ns_2048sl_1234s_doc_idx.npy
 > loading sample-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_test_indexmap_10ns_2048sl_1234s_sample_idx.npy
 > loading shuffle-idx mapping from /home/asc22g0/ASC22/my-gpt2/my-gpt2_text_sentence_test_indexmap_10ns_2048sl_1234s_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 66
    total number of epochs: 1
> finished creating GPT2 datasets ...
setting training data start iteration to 0
setting validation data start iteration to 0
done with setups ...
time (ms) | model and optimizer: 435.57 | train/valid/test data iterators: 550.61
training ...
 iteration        1/  500000 | elapsed time per iteration (ms): 703.5 | learning rate: 0.000E+00 | loss scale: 4294967296.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
after 1 iterations memory (MB) | allocated: 9046.81298828125 | max allocated: 11298.8134765625 | reserved: 12804.0 | max reserved: 12804.0
time (ms) | forward: 231.59 | backward: 471.42 | backward-backward: 422.08 | backward-allreduce: 39.00 | backward-master-grad: 10.19 | backward-clip-grad: 0.03 | optimizer: 0.05 | batch generator: 6.21
Effective Tera Flops per GPU: 27.5 and total parameters 4.722 B
 iteration        2/  500000 | elapsed time per iteration (ms): 608.0 | learning rate: 0.000E+00 | loss scale: 2147483648.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.49 | backward: 449.05 | backward-backward: 420.65 | backward-allreduce: 27.40 | backward-master-grad: 0.88 | backward-clip-grad: 0.02 | optimizer: 0.04 | batch generator: 1.02
Effective Tera Flops per GPU: 31.81 and total parameters 4.722 B
 iteration        3/  500000 | elapsed time per iteration (ms): 609.5 | learning rate: 0.000E+00 | loss scale: 1073741824.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.00 | backward: 452.09 | backward-backward: 420.62 | backward-allreduce: 27.66 | backward-master-grad: 3.70 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.78
Effective Tera Flops per GPU: 31.74 and total parameters 4.722 B
 iteration        4/  500000 | elapsed time per iteration (ms): 606.5 | learning rate: 0.000E+00 | loss scale: 536870912.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.18 | backward: 448.93 | backward-backward: 420.54 | backward-allreduce: 27.26 | backward-master-grad: 1.02 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.85
Effective Tera Flops per GPU: 31.89 and total parameters 4.722 B
 iteration        5/  500000 | elapsed time per iteration (ms): 606.5 | learning rate: 0.000E+00 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 154.95 | backward: 449.15 | backward-backward: 420.72 | backward-allreduce: 27.21 | backward-master-grad: 1.12 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.78
Effective Tera Flops per GPU: 31.89 and total parameters 4.722 B
 iteration        6/  500000 | elapsed time per iteration (ms): 606.5 | learning rate: 0.000E+00 | loss scale: 134217728.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.23 | backward: 448.85 | backward-backward: 420.43 | backward-allreduce: 27.30 | backward-master-grad: 1.02 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.85
Effective Tera Flops per GPU: 31.89 and total parameters 4.722 B
 iteration        7/  500000 | elapsed time per iteration (ms): 606.8 | learning rate: 0.000E+00 | loss scale: 67108864.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.06 | backward: 449.12 | backward-backward: 420.79 | backward-allreduce: 27.21 | backward-master-grad: 1.01 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.77
Effective Tera Flops per GPU: 31.88 and total parameters 4.722 B
 iteration        8/  500000 | elapsed time per iteration (ms): 613.9 | learning rate: 0.000E+00 | loss scale: 33554432.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.12 | backward: 456.33 | backward-backward: 428.00 | backward-allreduce: 27.26 | backward-master-grad: 0.97 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.84
Effective Tera Flops per GPU: 31.51 and total parameters 4.722 B
 iteration        9/  500000 | elapsed time per iteration (ms): 613.9 | learning rate: 0.000E+00 | loss scale: 16777216.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.01 | backward: 456.52 | backward-backward: 428.11 | backward-allreduce: 27.39 | backward-master-grad: 0.92 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.78
Effective Tera Flops per GPU: 31.51 and total parameters 4.722 B
 iteration       10/  500000 | elapsed time per iteration (ms): 616.7 | learning rate: 0.000E+00 | loss scale: 8388608.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.04 | backward: 459.29 | backward-backward: 428.13 | backward-allreduce: 30.16 | backward-master-grad: 0.89 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.81
Effective Tera Flops per GPU: 31.37 and total parameters 4.722 B
 iteration       11/  500000 | elapsed time per iteration (ms): 664.3 | learning rate: 0.000E+00 | loss scale: 4194304.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.04 | backward: 506.69 | backward-backward: 428.25 | backward-allreduce: 27.38 | backward-master-grad: 50.95 | backward-clip-grad: 0.02 | optimizer: 0.04 | batch generator: 0.78
Effective Tera Flops per GPU: 29.12 and total parameters 4.722 B
 iteration       12/  500000 | elapsed time per iteration (ms): 615.0 | learning rate: 0.000E+00 | loss scale: 2097152.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.40 | backward: 456.92 | backward-backward: 428.42 | backward-allreduce: 27.34 | backward-master-grad: 1.04 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.99
Effective Tera Flops per GPU: 31.45 and total parameters 4.722 B
 iteration       13/  500000 | elapsed time per iteration (ms): 614.7 | learning rate: 0.000E+00 | loss scale: 1048576.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.08 | backward: 457.17 | backward-backward: 428.48 | backward-allreduce: 27.22 | backward-master-grad: 1.37 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.78
Effective Tera Flops per GPU: 31.47 and total parameters 4.722 B
 iteration       14/  500000 | elapsed time per iteration (ms): 638.1 | learning rate: 0.000E+00 | loss scale: 524288.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
time (ms) | forward: 155.14 | backward: 480.64 | backward-backward: 428.25 | backward-allreduce: 27.29 | backward-master-grad: 25.00 | backward-clip-grad: 0.02 | optimizer: 0.03 | batch generator: 0.84
Effective Tera Flops per GPU: 30.31 and total parameters 4.722 B
 iteration       15/  500000 | elapsed time per iteration (ms): 1145.4 | learning rate: 4.687E-08 | lm loss: 1.149293E+01 | loss scale: 524288.0 | number of skipped iterations:   0 | number of nan iterations:   0 |
time (ms) | forward: 155.03 | backward: 581.54 | backward-backward: 428.18 | backward-allreduce: 27.31 | backward-master-grad: 68.39 | backward-clip-grad: 57.57 | optimizer: 406.40 | batch generator: 0.77
Effective Tera Flops per GPU: 16.89 and total parameters 4.722 B
